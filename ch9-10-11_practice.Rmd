---
title: "Chapters 9-11 Practice"
author: "Brandie Quarles"
date: "`r Sys.Date()`"
output: 
  html_document: 
    keep_md: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 2/12 Assignment

Return to the Chicago data and Q4 from the previous Chicago assignment.

For Q4 we were predicting ridership from weather PCs, station PCs, and
remaining predictors.

Use Cross-fold validation and the model selection techniques shown in
chapter 11 to compare models with:

1.  All PCs and remaining predictors (same as Q4)

    1.  optional: compare random forest and lm for this full data set

2.  The PCs + the weekend variable (no sports team data)

3.  1 weather PC, 1 station PC, + the weekend variable

4.  1 weather PC + the weekend variable

5.  1 station PC + the weekend variable

6.  The weekend variable only

7.  Feel free to add additional models if there are things you want to
    test

## Load necessary packages 
```{r}
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
library(timeDate)
library(modeldata)
```

## Load the Data
```{r}
data(Chicago, package = "modeldata")
head(Chicago)
```

## Here is how Julin set up the weekend variable:
```{r}
Chicago <- Chicago %>%
  mutate(weekend = timeDate::isBizday(timeDate::as.timeDate(date)))
```

## Cross Fold Validation
```{r}
set.seed(010324)
chicago_split <- initial_split(Chicago, prop = 0.80, strata = ridership)
#could also have stratified by weekend
chicago_train <- training(chicago_split)
chicago_test <- testing(chicago_split)

chicago_folds <- vfold_cv(chicago_train, v = 10)
chicago_folds
```

## Recipes for the different models
```{r}
chicago_basic_recipe <- recipe(ridership ~ ., 
         data = chicago_train) %>%
  update_role(date, new_role = "date") %>% #need to get rid of the column date so it's not a predictor 
  add_role(matches("Home|Away$"), new_role="sports") %>%
  add_role(matches("^[A-Z]", ignore.case=FALSE), -has_role("sports"), new_role="station") %>%
  add_role(matches("^[a-z]", ignore.case=FALSE), -date, -weekend, -ridership, new_role="weather") %>% 
  step_normalize(has_role("weather"), has_role("station")) 

chicago_full_rec <-
    chicago_basic_recipe %>%
  step_pca(has_role("station"), threshold = 0.75, prefix = "S_PC", id="station_pca")  %>%
  step_pca(has_role("weather"), threshold = 0.75, prefix = "W_PC", id = "weather_pca") 

allPCs_rec <-
  chicago_full_rec %>% 
  step_rm(has_role("sports"))
allPCs_rec %>% prep() %>% bake(chicago_train[1:6,])

station_weatherPC_rec <- 
  chicago_basic_recipe %>% 
  step_rm(has_role("sports")) %>% 
  step_pca(Austin:California, 
           prefix = "St_PC", num_comp=1, id = "station_pca") %>% 
  step_pca(temp_min:weather_storm, 
           prefix = "W_PC", num_comp=1, id = "weather_pca")
station_weatherPC_rec %>% prep() %>% bake(chicago_train[1:6,])

weatherPC_rec <- 
  chicago_basic_recipe %>%
  step_rm(has_role("sports")) %>%
  step_rm(has_role("station")) %>%
  step_pca(temp_min:weather_storm, 
           prefix = "W_PC", num_comp=1, id = "weather_pca")
weatherPC_rec %>% prep() %>% bake(chicago_train[1:6,])

stationPC_rec <- 
  chicago_basic_recipe %>%
  step_rm(has_role("sports")) %>%
  step_rm(has_role("weather")) %>%
  step_pca(Austin:California, 
           prefix = "St_PC", num_comp=1, id = "station_pca")
tidy(stationPC_rec)
stationPC_rec %>% prep() %>% bake(chicago_train[1:6,])

weekend_recipe <-
  recipe(ridership ~ weekend,
         data = chicago_train)

preproc <- 
  list(full = chicago_full_rec, 
       allpcs = allPCs_rec,
       st_w_pc = station_weatherPC_rec,
       w_pc = weatherPC_rec, 
       st_pc = stationPC_rec,
       weekend = weekend_recipe
  )

lm_models <- workflow_set(preproc, list(lm = linear_reg()), cross = FALSE) #combine the recipes into a single workflow set
lm_models
```

## Resample each model 
```{r}
keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

lm_models <- 
  lm_models %>% 
  workflow_map("fit_resamples", 
               # Options to `workflow_map()`: 
               seed = 1102, #makes sure each model uses the same random number stream 
               verbose = TRUE, #prints the progress 
               # Options to `fit_resamples()`: 
               resamples = chicago_folds,
               control = keep_pred) #saves the predictions and workflows from the assessment set

lm_models
#full_lm warning: prediction from rank-deficient fit; consider predict(., rankdeficient="NA")
lm_models$option[1]
```

## Compare Models 
```{r}
collect_metrics(lm_models) %>% 
  filter(.metric == "rmse") #smaller value is better
#measure the difference b/t the predicted and observed values

collect_metrics(lm_models) %>% 
  filter(.metric == "rsq") #closer to 1 is better
#squared correlation b/t the predicted and observed values

#first three models have pretty similar performance (better than the other models)


```

## Add Random Forest Model (of full recipe only)
```{r}
rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wflow <- 
  workflow() %>% 
  add_recipe(chicago_full_rec) %>% 
  add_model(rf_model) 

set.seed(1003)
rf_res <- rf_wflow %>% fit_resamples(resamples = chicago_folds, control = keep_pred)

seven_models <- 
  as_workflow_set(random_forest = rf_res) %>% 
  bind_rows(lm_models)
seven_models
```

## Plot R^2^
```{r}
library(ggrepel)
autoplot(seven_models, metric = "rsq") +
  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")

#random forest seems to be best?
```

## Comparing Resampled Performance Statistically
Plot the different resamples 
```{r}
rsq_indiv_estimates <- 
  collect_metrics(seven_models, summarize = FALSE) %>% 
  filter(.metric == "rsq") #filter to get only R2

rsq_wider <- 
  rsq_indiv_estimates %>% 
  select(wflow_id, .estimate, id) %>% 
  pivot_wider(id_cols = "id", names_from = "wflow_id", values_from = ".estimate")

rsq_indiv_estimates %>% 
  mutate(wflow_id = reorder(wflow_id, .estimate)) %>% 
  ggplot(aes(x = wflow_id, y = .estimate, group = id, color = id)) + 
  geom_line(alpha = .5, linewidth = 1.25) + #lines connect the resamples 
  theme(legend.position = "none")
#mostly parallel but a few criss cross... --> Assume the resamples only change the intercept of the model?
```

Bayesian Random Intercept Model 
```{r}
library(tidyposterior)
library(rstanarm)

rsq_anova <-
  perf_mod(
    seven_models,
    metric = "rsq",
    prior_intercept = rstanarm::student_t(df = 1), #use default priors except for the random intercepts 
    chains = 4,
    iter = 5000, #tells the function how long to run the estimation process in each chain 
    seed = 1102 #estimation process uses random numbers 
  )
```

Extract Posterior Distributions
```{r}
model_post <- 
  rsq_anova %>% 
  # Take a random sample from the posterior distribution
  # so set the seed again to be reproducible. 
  tidy(seed = 1103) 

glimpse(model_post)
```

Plot distributions
```{r}
model_post %>% 
  mutate(model = forcats::fct_inorder(model)) %>%
  ggplot(aes(x = posterior)) + 
  geom_histogram(bins = 50, color = "white", fill = "blue", alpha = 0.4) + 
  xlab("Posterior for mean Rsq") +
  facet_wrap(~ model, ncol = 1)

autoplot(rsq_anova) +
  geom_text_repel(aes(label = workflow), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")

autoplot(model_post) #overlaid density plots from the tidied object 
```

