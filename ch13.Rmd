---
title: "Chapter 13"
author: "Brandie Quarles"
date: "`r Sys.Date()`"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 13 - Grid Search

<https://www.tmwr.org/grid-search>

## Libraries

```{r}
library(tidymodels)
tidymodels_prefer()
library(doMC) #for parallelization 
registerDoMC(cores = 3) #1 less core than I have available 
```

### Previous AMES Code:

```{r}
registerDoMC(cores = 7)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)

lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)

rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
      Latitude + Longitude) %>% 
  add_model(rf_model) 

set.seed(1001)
ames_folds <- vfold_cv(ames_train, v = 10)

keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

set.seed(1003)
rf_res <- rf_wflow %>% fit_resamples(resamples = ames_folds, control = keep_pred)
```

Notes:

-   Usemodels gave different/wrong results when I ran it, so use that with caution

-   Should use parallelization for conducting the grid (see Julin's code)

-   Typo about Figure 13.4, not opposite results from previous figure

## Regular and NonRegular Grids

Practice with the multilayer perception model (single layer artificial neural network)

Parameters for tuning:

-   \# hidden units

-   \# of fitting epochs/iterations in model training

-   amount of weight decay penalization

```{r}
library(tidymodels)
tidymodels_prefer()

mlp_spec <- 
  mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% 
  set_engine("nnet", trace = 0) %>% 
  set_mode("classification")
```

Extract the arguments w/ unknown values (parameters for tuning) and set their dials objects:

```{r}
mlp_param <- extract_parameter_set_dials(mlp_spec)
mlp_param %>% extract_parameter_dials("hidden_units")

mlp_param %>% extract_parameter_dials("penalty")

mlp_param %>% extract_parameter_dials("epochs")

```

### Regular Grids - combines each parameter factorially (uses all combos of the sets)

1.  Create a distinct set of values for each parameter. The \# of possible values doesn't have to be the same for each parameter

```{r}
crossing(
  hidden_units = 1:3,
  penalty = c(0.0, 0.1),
  epochs = c(100, 200)
)
```

2.  Produce the grid

```{r}
grid_regular(mlp_param, 
             levels = 2) # number of levels per parameter to create 
```

```{r}
mlp_param %>% 
  grid_regular(levels = c(hidden_units = 3, penalty = 2, epochs = 2)) #alternative way to use levels 
```

Alternative method = fractional factorial designs (don't use all possible values of each set)

Potential downside: can be computationally expensive to use (esp. when there are medium-large \# of tuning parameters

Advantages:

-   many models whose tuning time decreases w/ a regular grid

-   relats and patterns b/t the tuning parameters & the model metrics are easily understood (little confounding b/t parameters)

### Irregular Grids - parameter combos are not formed from a small set of points

#### Option 1 = use random sampling across the range of parameters.

1.  generate independent uniform random #s across the parameter ranges. If the parameter has a transformation it will generate them on the transformed scale

```{r}
set.seed(1301)
mlp_param %>% 
  grid_random(size = 1000) %>% # 'size' is the number of combinations
  summary()
```

Potential problems:

-   w/ small-medium grids, random values can result in overlapping parameter combos.

-   you need the grid to cover the whole parameter space, but the likelihood of that is lower w/ fewer grid values

```{r}
library(ggforce)
set.seed(1302)
mlp_param %>% 
  # The 'original = FALSE' option keeps penalty in log10 units
  grid_random(size = 20, original = FALSE) %>% 
  ggplot(aes(x = .panel_x, y = .panel_y)) + 
  geom_point() +
  geom_blank() +
  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + 
  labs(title = "Random design with 20 candidates")
#Can see some overlap in points 
```

#### Option 2 = space-filling designs: find a configuration of points that cover the parameter space w/ the smallest change of overlapping or redundant values

```{r}
set.seed(1303)
mlp_param %>% 
  grid_latin_hypercube(size = 20, original = FALSE) %>% 
  ggplot(aes(x = .panel_x, y = .panel_y)) + 
  geom_point() +
  geom_blank() +
  facet_matrix(vars(hidden_units, penalty, epochs), layer.diag = 2) + 
  labs(title = "Latin Hypercube design with 20 candidates")

#less overlap in points and better exploration of the parameter space
```

## Evaluating the Grid

Each candidate set should be assessed using non-training data --\> resampling or validation methods needed

Then choose the best set: either the empirically best or simplest

Practice w/ dataset from automated microscopy lab tool for cancer research

```{r}
library(tidymodels)
data(cells)
cells <- cells %>% select(-case) #remove column not needed for analysis
```

Use 10-fold CV

```{r}
set.seed(1304)
cell_folds <- vfold_cv(cells)
```

High correlation b/t predictors --\> use PCA feature extraction

```{r}
mlp_rec <-
  recipe(class ~ ., data = cells) %>%
  step_YeoJohnson(all_numeric_predictors()) %>% #encourage more symmetric distribution of predictors (counter the skewed dist. before PCA)
  step_normalize(all_numeric_predictors()) %>% #normalize to be on same scale prior to PCA
  step_pca(all_numeric_predictors(), num_comp = tune()) %>% #PCA w/ tuning for # of components to retain 
  step_normalize(all_numeric_predictors()) #normalize again to corce predictors to have same mean and variance (lower rank components tend to have a wider range than higher-rank components)

mlp_wflow <- 
  workflow() %>% 
  add_model(mlp_spec) %>% 
  add_recipe(mlp_rec)
```

Create a parameter object

```{r}
mlp_param <- 
  mlp_wflow %>% 
  extract_parameter_set_dials() %>% 
  update(
    epochs = epochs(c(50, 200)),
    num_comp = num_comp(c(0, 40)) #0 PCA components = shortcut for comparing original predictors to PCA results 
  )
```

Conduct the grid search

```{r}
roc_res <- metric_set(roc_auc)
set.seed(1305)
mlp_reg_tune <-
  mlp_wflow %>% #model specificaiton or workflow = 1st argument 
  tune_grid( #similar to fit_resamples()
    cell_folds, #resampling object 
    grid = mlp_param #integer or data frame to pass to tune function
    #if grid is integer can use param_info to define the ranges 
    %>% grid_regular(levels = c(hidden_units=3, penalty=5, epochs=3, num_comp=3)), #regular grid w/ 3 levels across resamples 
    metrics = roc_res #measure area under the ROC curve during resampling 
  )
mlp_reg_tune #this took a really long time to run for me 
#paralleizaiton maybe helped, but not drastically since I only have 3 cores to spare 
```

Plot performance profiles across tuning parameters:

```{r}
autoplot(mlp_reg_tune) + 
  scale_color_viridis_d(direction = -1) + 
  theme(legend.position = "top")
#the amount of penalization has the largest impact on the area under the ROC curve. 
#number of hidden units appears to matter most when the amount of regularization is low (and harms performance)
```

Show numerically best results:

```{r}
show_best(mlp_reg_tune) %>% select(-.estimator)
#several configs w/ similar performance 
```

Follow up with space-filling design to run the grid search w/ larger values of weight decay penalty

```{r}
set.seed(1306)
mlp_sfd_tune <-
  mlp_wflow %>%
  tune_grid(
    cell_folds,
    grid = 20,
    # Pass in the parameter object to use the appropriate range: 
    param_info = mlp_param,
    metrics = roc_res
  )
mlp_sfd_tune
```

```{r}
autoplot(mlp_sfd_tune) #marginal effects plot
#note: values of the other tuning parameters can affect each panel.
```

```{r}
show_best(mlp_sfd_tune) %>% select(-.estimator)
```

Other notes: "The `extract` option to `control_grid()` allows the retention of the fitted models and/or recipes. Also, setting the `save_pred` option to `TRUE` retains the assessment set predictions and these can be accessed using `collect_predictions()`."

## Finalizing the Model

tune_grid() does not fit a final model, just helps you choose the appropriate tuning parameters.

Pick a final set of parameter values by:

-   manually picking values that seem appropriate from show_best() or

-   use a select\_\*() function

`select_best()` will choose the parameters with the numerically best results

```{r}
select_best(mlp_reg_tune, metric = "roc_auc")
```

Looking back at show_best from above, you can see there's a simpler option that performs similar to the one above

```{r}
logistic_param <-  #create tibble with the values of the simpler model 
  tibble(
    num_comp = 0,
    epochs = 125,
    hidden_units = 1,
    penalty = 1
  )

final_mlp_wflow <- 
  mlp_wflow %>% 
  finalize_workflow(logistic_param) #splice the values back into the workflow 
final_mlp_wflow
```

Fit model on entire training set:

```{r}
final_mlp_fit <- 
  final_mlp_wflow %>% 
  fit(cells)
```

\*Note: "If you did not use a workflow, finalization of a model and/or recipe is done using `finalize_model()` and `finalize_recipe()"`

## Tools for Creating Tuning Specifications

"The **usemodels** package can take a data frame and model formula, then write out R code for tuning the model."

```{r}
library(usemodels)

use_xgboost(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
              Latitude + Longitude, 
            data = ames_train,
            # Add comments explaining some of the code:
            verbose = TRUE)
```

\*Note: "The **usemodels** package can also be used to create model fitting code with no tuning by setting the argument `tune = FALSE`."

## Tools for Efficient Grid Search

### Submodel Optimization

Some models where multiple tuning parameters can be evaluated w/o refitting

Ex: "a PLS model created with 100 components can also make predictions for any `num_comp <= 100"`

See Section 13.5.1 for more examples.

### Parallel Processing

Can decrease execution time when resampling models.

See Section 13.5.2 for things to consider when deciding how to parallelize

### Benchmarking Boosted Trees

See section 13.5.3 for info.

### Access to Global Variables

"When using tidymodels, it is possible to use values in your local environment (usually the global environment) in model objects."

See section 13.5.4 for more info.

### Racing Methods

See section 13.5.5 for info.
