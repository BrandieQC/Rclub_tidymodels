---
title: "Chapter 15"
author: "Brandie Quarles"
date: "`r Sys.Date()`"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 15 - Screening Many Models

<https://www.tmwr.org/workflow-sets>

For projects w/ new data sets, you may need to screen many combos of models and preprocessors.

Good strategy:

1.  Spend time trying a variety of modeling approaches
2.  Tweak/optimize a small set of models that worked best from step 1.

## Modeling Concrete Mixture Strength

Goal: predict the compressive strength of concrete mixtures using the ingredients as predictors

Load the relevant packages and data:

```{r}
library(tidymodels)
tidymodels_prefer()
library(doMC) #for parallelization 
registerDoMC(cores = 2) #using 2 cores b/c 3 or 4 was super slow. see note below
data(concrete, package = "modeldata")
glimpse(concrete)
```

Since there are multiple replicates for some concrete formulas, use the mean compressive strength for modeling.

\*Note: This seems like the wrong approach for real data where you want to keep the replicates in the model for power...

\*Note about parallelization: "For each of these technologies, the memory requirements multiply for each additional core used. For example, if the current data set is 2 GB in memory and three cores are used, the total memory requirement is 8 GB (2 for each worker process plus the original). Using too many cores might cause the computations (and the computer) to slow considerably." I only have 8 GB of memory, so fewer cores is probably better

```{r}
concrete <- concrete %>% group_by(across(-compressive_strength)) %>% summarize(compressive_strength = mean(compressive_strength), .groups = "drop") 
nrow(concrete) 
```

Define data splitting and resampling:

```{r}
set.seed(1501)
concrete_split <- initial_split(concrete, strata = compressive_strength) #using standard 3:1 ratio for training to test 
concrete_train <- training(concrete_split)
concrete_test  <- testing(concrete_split) 

set.seed(1502)
concrete_folds <- 
   vfold_cv(concrete_train, strata = compressive_strength, repeats = 5) #10 fold CV for the trianing set, repeats the 10-fold partitioning 5 times 
```

Create 2 recipes:

1.  for models that need predictors to e centered and scaled
2.  for traditional models (i.e. quadratic and two-way interactions)

```{r}
normalized_rec <- 
   recipe(compressive_strength ~ ., data = concrete_train) %>% 
   step_normalize(all_predictors()) 

poly_recipe <- 
   normalized_rec %>% 
   step_poly(all_predictors()) %>% 
   step_interact(~ all_predictors():all_predictors())
```

Create a set of model specs.

```{r}
#parsnip pacakge
library(rules) 
library(baguette)

linear_reg_spec <- 
   linear_reg(penalty = tune(), mixture = tune()) %>% 
   set_engine("glmnet")

nnet_spec <- 
   mlp(hidden_units = tune(), penalty = tune(), epochs = tune()) %>% 
   set_engine("nnet", MaxNWts = 2600) %>% 
   set_mode("regression")

mars_spec <- 
   mars(prod_degree = tune()) %>%  #<- use GCV to choose terms
   set_engine("earth") %>% 
   set_mode("regression")

svm_r_spec <- 
   svm_rbf(cost = tune(), rbf_sigma = tune()) %>% 
   set_engine("kernlab") %>% 
   set_mode("regression")

svm_p_spec <- 
   svm_poly(cost = tune(), degree = tune()) %>% 
   set_engine("kernlab") %>% 
   set_mode("regression")

knn_spec <- 
   nearest_neighbor(neighbors = tune(), dist_power = tune(), weight_func = tune()) %>% 
   set_engine("kknn") %>% 
   set_mode("regression")

cart_spec <- 
   decision_tree(cost_complexity = tune(), min_n = tune()) %>% 
   set_engine("rpart") %>% 
   set_mode("regression")

bag_cart_spec <- 
   bag_tree() %>% 
   set_engine("rpart", times = 50L) %>% 
   set_mode("regression")

rf_spec <- 
   rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>% 
   set_engine("ranger") %>% 
   set_mode("regression")

xgb_spec <- 
   boost_tree(tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), 
              min_n = tune(), sample_size = tune(), trees = tune()) %>% 
   set_engine("xgboost") %>% 
   set_mode("regression")

cubist_spec <- 
   cubist_rules(committees = tune(), neighbors = tune()) %>% 
   set_engine("Cubist") 
```

Neural network model requires 27 hidden units:

```{r}
nnet_param <- 
   nnet_spec %>% 
   extract_parameter_set_dials() %>% 
   update(hidden_units = hidden_units(c(1, 27)))
```

## Creating the Workflow Set

Reminder: workflow sets take named lists of preprocessors and model specs --\> object w/ multiple workflows

Combine the recipe that standardizes the predictors to the nonlinear models that require the predictors to be in the same units:

```{r}
normalized <- 
   workflow_set(
      preproc = list(normalized = normalized_rec), 
      models = list(SVM_radial = svm_r_spec, SVM_poly = svm_p_spec, 
                    KNN = knn_spec, neural_network = nnet_spec)
   )
normalized
```

"Since there is only a single preprocessor, this function creates a set of workflows with this value. If the preprocessor contained more than one entry, the function would create all combinations of preprocessors and models."

Can change the name of the workflow_id with mutate()

Extract a single workflow to get more info:

```{r}
normalized %>% extract_workflow(id = "normalized_KNN")
```

Option column = placeholder for any arguments to use when evaluating the workflow (i.e. for tuning).

```{r}
normalized <- 
   normalized %>% 
   option_add(param_info = nnet_param, id = "normalized_neural_network")
normalized
```

Result col = placeholder for output of tuning or resampling

Create another workflow set for other nonlinear models

```{r}
model_vars <- 
   workflow_variables(outcomes = compressive_strength, 
                      predictors = everything())

no_pre_proc <- 
   workflow_set(
      preproc = list(simple = model_vars), 
      models = list(MARS = mars_spec, CART = cart_spec, CART_bagged = bag_cart_spec,
                    RF = rf_spec, boosting = xgb_spec, Cubist = cubist_spec)
   )
no_pre_proc
```

Create a workflow set that uses nonlinear terms and interactions:

```{r}
with_features <- 
   workflow_set(
      preproc = list(full_quad = poly_recipe), 
      models = list(linear_reg = linear_reg_spec, KNN = knn_spec)
   )
```

Can combine all the above workflow sets into one large tibble:

```{r}
all_workflows <- 
   bind_rows(no_pre_proc, normalized, with_features) %>% 
   # Make the workflow ID's a little more simple: 
   mutate(wflow_id = gsub("(simple_)|(normalized_)", "", wflow_id))
all_workflows
```

\*Note: That's cool!

## Tuning and Evaluating the Models

Can use workflow_map() to apply the same tuning functions to all the models in the mega workflow set

```{r, eval=FALSE}
grid_ctrl <-
   control_grid(
      save_pred = TRUE,
      parallel_over = "everything",
      save_workflow = TRUE
   )

sleep_for_a_minute <- function() { Sys.sleep(60) }

start_time <- Sys.time()
# Time difference of 1.000327 mins
grid_results <-
   all_workflows %>%
   workflow_map(
      seed = 1503, #ensures each execution of tune_grid() consumes the same random numbers 
      resamples = concrete_folds,
      grid = 25,
      control = grid_ctrl
   )
end_time <- Sys.time()
end_time - start_time
#âœ– The workflow requires packages that are not installed: 'xgboost'. Skipping this workflow.
#Timing stopped at: 9878 56.51 7615
#Execution stopped; returning current results
#The above was taking a really long time to run (more than 4 hours so I stopped it)
#I left this overnight and it still didn't run

grid_results #can see the option and results cols have been updated
```

"In the `result` columns, the"`tune[+]`" and "`rsmp[+]`" notations mean that the object had no issues. A value such as "`tune[x]`" occurs if all of the models failed for some reason."

Examining results:

```{r, eval=FALSE}
grid_results %>% 
   rank_results() %>% 
   filter(.metric == "rmse") %>% 
   select(model, .config, rmse = mean, rank) #orders the models by a rmse (can also use a different performance metric ) 

#Error in `halt()`:
#! There were 6 workflows that had no results.
```

"Also by default, the function ranks all of the candidate sets; that\'s why the same model can show up multiple times in the output. An option, called `select_best`, can be used to rank the models using their best tuning parameter combination."

Plot the rankings:

```{r, eval=FALSE}
autoplot(
   grid_results,
   rank_metric = "rmse",  # <- how to order models
   metric = "rmse",       # <- which metric to visualize
   select_best = TRUE     # <- one point per workflow (chooses the best turning parameter combo for that workflow)
) +
   geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1) +
   lims(y = c(3.5, 9.5)) +
   theme(legend.position = "none")
```

Can look at the tuning results or a spec. model with id argument:

```{r, eval=FALSE}
autoplot(grid_results, id = "Cubist", metric = "rmse")
```

Can also collect_predictions() and collect_metrics(), but they didn't provide an example of that code.

## Efficiently Screening Models

Can using the racing approach described in chapter 13 to screen a large set of models.

```{r, eval=FALSE}
library(finetune)

race_ctrl <-
   control_race(
      save_pred = TRUE,
      parallel_over = "everything",
      save_workflow = TRUE
   )

race_results <-
   all_workflows %>%
   workflow_map( #map the function to all workflows 
      "tune_race_anova",
      seed = 1503,
      resamples = concrete_folds,
      grid = 25,
      control = race_ctrl
   )

race_results #now the results column has "race[+} instead of tune[+]
```

Can use same methods as previous section for interrogating results:

```{r, eval=FALSE}
autoplot(
   race_results,
   rank_metric = "rmse",  
   metric = "rmse",       
   select_best = TRUE    
) +
   geom_text(aes(y = mean - 1/2, label = wflow_id), angle = 90, hjust = 1) +
   lims(y = c(3.0, 9.5)) +
   theme(legend.position = "none")
```

"Overall, the racing approach estimated a total of 1,050 models, 8.33% of the full set of 12,600 models in the full grid. As a result, the racing approach was 4.8-fold faster."

Check if you get similar results b/t the different methods (complete grid vs. racing grid):

```{r, eval=FALSE}
matched_results <- 
   rank_results(race_results, select_best = TRUE) %>% 
   select(wflow_id, .metric, race = mean, config_race = .config) %>% 
   inner_join(
      rank_results(grid_results, select_best = TRUE) %>% 
         select(wflow_id, .metric, complete = mean, 
                config_complete = .config, model),
      by = c("wflow_id", ".metric"),
   ) %>%  
   filter(.metric == "rmse")

library(ggrepel)

matched_results %>% 
   ggplot(aes(x = complete, y = race)) + 
   geom_abline(lty = 3) + 
   geom_point() + 
   geom_text_repel(aes(label = model)) +
   coord_obs_pred() + 
   labs(x = "Complete Grid RMSE", y = "Racing RMSE") 
```

"While the racing approach selected the same candidate parameters as the complete grid for only 41.67% of the models, the performance metrics of the models selected by racing were nearly equal. The correlation of RMSE values was 0.968 and the rank correlation was 0.951. This indicates that, within a model, there were multiple tuning parameter combinations that had nearly identical results."

## Finalizing a Model

1.  Pick a workflow to finalize
2.  Update the parameters w/ the numerically best settings
3.  Fit to training set

```{r, eval=FALSE}
best_results <- 
   race_results %>% 
   extract_workflow_set_result("boosting") %>% 
   select_best(metric = "rmse")
best_results


boosting_test_results <- 
   race_results %>% 
   extract_workflow("boosting") %>% 
   finalize_workflow(best_results) %>% 
   last_fit(split = concrete_split)
```

4.  Look at the test set metrics results

```{r, eval=FALSE}
collect_metrics(boosting_test_results)
```

5.  Visualize predictions

```{r, eval=FALSE}
boosting_test_results %>% 
   collect_predictions() %>% 
   ggplot(aes(x = compressive_strength, y = .pred)) + 
   geom_abline(color = "gray50", lty = 2) + 
   geom_point(alpha = 0.5) + 
   coord_obs_pred() + 
   labs(x = "observed", y = "predicted")
```
