---
title: "Chapter 11"
author: "Brandie Quarles"
date: "`r Sys.Date()`"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 11 - Comparing Models with Resampling

Notes from meeting

-   Regularly accepted method of model comparison. Likelihood ratio test - chisq test tells you if the likelihoods are significantly different

    -   This would be comparing two models at a time though

    -   \--\> the method of putting rsqs into an ANOVA is conceptually similar so shouldn't be an issue of using your data in analysis twice

-   Practical effect size

    -   Use intuition to set effect size that would be biologically significant

    -   If 1% better but takes 2x times to run, pick the quicker model

    -   If two models are basically the same, choose the simplest model

<https://www.tmwr.org/compare>

### Previous AMES Code:

```{r}
library(tidymodels)
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)

lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)

rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
      Latitude + Longitude) %>% 
  add_model(rf_model) 

set.seed(1001)
ames_folds <- vfold_cv(ames_train, v = 10)

keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

set.seed(1003)
rf_res <- rf_wflow %>% fit_resamples(resamples = ames_folds, control = keep_pred)
```

## Creating Multiple Models with Workflow Sets

Create 3 different linear models that add preprocessing steps incrementally --\> test whether additional terms improve the model results

```{r}
library(tidymodels)
tidymodels_prefer()

basic_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors())

interaction_rec <- 
  basic_rec %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) 

spline_rec <- 
  interaction_rec %>% 
  step_ns(Latitude, Longitude, deg_free = 50)

preproc <- 
  list(basic = basic_rec, 
       interact = interaction_rec, 
       splines = spline_rec
  )

lm_models <- workflow_set(preproc, list(lm = linear_reg()), cross = FALSE) #combine the recipes into a single workflow set
lm_models
```

Resample each model in turn

workflow_map() takes an initial argument = function to apply to the workflows, options to that function follow that argument

```{r}
lm_models <- 
  lm_models %>% 
  workflow_map("fit_resamples", 
               # Options to `workflow_map()`: 
               seed = 1101, #makes sure each model uses the same random number stream 
               verbose = TRUE, #prints the progress 
               # Options to `fit_resamples()`: 
               resamples = ames_folds, #give it the 10-fold partitioned Ames data from ch10
               control = keep_pred) #give it the control_resamples from ch10 that saves the predictions and workflows from the assessment set

lm_models
lm_models$option[1]
```

Use collect_metrics() to collate the performance stats

```{r}
collect_metrics(lm_models) %>% 
  filter(.metric == "rmse")
```

Add the random forest model

```{r}
four_models <- 
  as_workflow_set(random_forest = rf_res) %>% 
  bind_rows(lm_models)
four_models
```

Plot R^2^

```{r}
library(ggrepel)
autoplot(four_models, metric = "rsq") +
  geom_text_repel(aes(label = wflow_id), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")
```

## Comparing Resampled Performance Stats

Resample-to-resample component of variation - results for the same resample tend to be similar (i.e. some resamples/fold where performance across models tends to be low and others high).

```{r}
rsq_indiv_estimates <- 
  collect_metrics(four_models, summarize = FALSE) %>% 
  filter(.metric == "rsq") #filter to get only R2

rsq_wider <- 
  rsq_indiv_estimates %>% 
  select(wflow_id, .estimate, id) %>% 
  pivot_wider(id_cols = "id", names_from = "wflow_id", values_from = ".estimate")

corrr::correlate(rsq_wider %>% select(-id), quiet = TRUE)
```

The correlations are high --\> across models, there are large w/in-resample corrs

Plot it:

```{r}
rsq_indiv_estimates %>% 
  mutate(wflow_id = reorder(wflow_id, .estimate)) %>% 
  ggplot(aes(x = wflow_id, y = .estimate, group = id, color = id)) + 
  geom_line(alpha = .5, linewidth = 1.25) + #lines connect the resamples 
  theme(legend.position = "none")
```

Correlations --\> parallel lines

```{r}
rsq_wider %>% 
  with( cor.test(basic_lm, splines_lm) ) %>% 
  tidy() %>% 
  select(estimate, starts_with("conf"))
```

Confidence seem to imply that the correlations are real/significant.

\*Ignoring the resample-to-resample effect would bias the model comparisons to find no diffs b/t models.

**Practical effect size** - the change in the performance statistic of interest that would be considered a realistic difference that matters

-   This is of course very subjective

-   Ex: R^2^ that are within +/-2% would not be "practically" different

## Simple Hypothesis Testing Methods

Can use an ANOVA to make model comparisons. The resampling statistic would serve as the outcome data and the models as the predictors.

How to deal with resample effects?

-   Treat them as random effects

-   Use the differences in R^2^ values as the outcome data in the ANOVA. Since the outcomes are matched by resample, the diffs don't contain the resample effect. Can only compare two models at a time though.

```{r}
compare_lm <- 
  rsq_wider %>% 
  mutate(difference = splines_lm - basic_lm)

lm(difference ~ 1, data = compare_lm) %>% 
  tidy(conf.int = TRUE) %>% 
  select(estimate, p.value, starts_with("conf"))

# Alternatively, a paired t-test could also be used: 
rsq_wider %>% 
  with( t.test(splines_lm, basic_lm, paired = TRUE) ) %>%
  tidy() %>% 
  select(estimate, p.value, starts_with("conf"))
```

## Bayesian Methods

Need *prior distribution* specifications for the model parameters.

-   The model assumes these before being exposed to the observed data.

-   Set the possible/probable ranges of the model parameters and have no unknown parameters

-   "In many cases, we might not have a strong opinion about the prior beyond it being symmetric and bell shaped." Large standard deviation ---\> fairly uninformative prior

    -   not overly restrictive in terms of the possible values that the parameters might take on ---\> data can have more of an influence during parameter estimation

Final distributions of the model parameters are combos of the priors and likelihood estimates ---\> *posterior distributions*

-   Full probabilistic desc of the model's estimated parameters

### Random Intercept Model

Assume the resamples only change the intercept of the model.

-   Makes sense for this example since the effects seemed to be mostly parallel when we plotted them above (didn't change the slope too much)

tidyposterior package has functions to fit Bayesian models to compare resampled models

-   fits models with the rstanarm package.

    -   has default priors (see ?priors)

    -   estimation process is iterative and replicated several times in *chains*

<!-- -->

-   perf_mod() = main function

    -   for workflows: creates an ANOVA where groups correspond to the workflows

    -   single models: makes w/in model comparisons; groups = submodels defined by tuning parameters

    -   data frame produced by rsample that has columns of performance metrics associated w/ 2 or more model/workflow results

```{r}
library(tidyposterior)
library(rstanarm)

# The rstanarm package creates copious amounts of output; those results
# are not shown here but are worth inspecting for potential issues. The
# option `refresh = 0` can be used to eliminate the logging. 
rsq_anova <-
  perf_mod(
    four_models,
    metric = "rsq",
    prior_intercept = rstanarm::student_t(df = 1), #use default priors excpet for the random intercepts 
    chains = 4,
    iter = 5000, #tells the functin how long to run the estimation proess in each chain 
    seed = 1102 #estimation process uses random numbers 
  )
```

Use tidy to extract posterior distributions

```{r}
model_post <- 
  rsq_anova %>% 
  # Take a random sample from the posterior distribution
  # so set the seed again to be reproducible. 
  tidy(seed = 1103) 

glimpse(model_post)
```

Plot the distributions

```{r}
model_post %>% 
  mutate(model = forcats::fct_inorder(model)) %>%
  ggplot(aes(x = posterior)) + 
  geom_histogram(bins = 50, color = "white", fill = "blue", alpha = 0.4) + 
  xlab("Posterior for mean Rsq") +
  facet_wrap(~ model, ncol = 1)
```

```{r}
autoplot(rsq_anova) +
  geom_text_repel(aes(label = workflow), nudge_x = 1/8, nudge_y = 1/100) +
  theme(legend.position = "none")
```

```{r}
autoplot(model_post) #overlaid density plots from the tidied object 
```

Compare models

-   Start with the basic vs. spline linear model

```{r}
rqs_diff <-
  contrast_models(rsq_anova,
                  list_1 = "splines_lm",
                  list_2 = "basic_lm",
                  seed = 1104)

rqs_diff %>% 
  as_tibble() %>% 
  ggplot(aes(x = difference)) + 
  geom_vline(xintercept = 0, lty = 2) + 
  geom_histogram(bins = 50, color = "white", fill = "red", alpha = 0.4)
```

"The posterior shows that the center of the distribution is greater than zero (indicating that the model with splines typically had larger values) but does overlap with zero to a degree."

Compute the mean of the above distribution and the Bayesian analog to confidence intervals (credible intervals):

```{r}
summary(rqs_diff) %>% 
  select(-starts_with("pract"))
```

"The `probability` column reflects the proportion of the posterior that is greater than zero. This is the probability that the positive difference is real. The value is not close to zero, providing a strong case for statistical significance, i.e., the idea that statistically the actual difference is not zero."

"However, the estimate of the mean difference is fairly close to zero. Recall that the practical effect size we suggested previously is 2%. With a posterior distribution, we can also compute the probability of being practically significant. In Bayesian analysis, this is a *ROPE estimate* (for Region Of Practical Equivalence, Kruschke and Liddell ([2018](https://www.tmwr.org/compare#ref-kruschke2018bayesian))). To estimate this, the `size` option to the summary function is used:"

```{r}
summary(rqs_diff, size = 0.02) %>% 
  select(contrast, starts_with("pract"))
```

pract_equiv column = prop of posterior w/in the ROPE. Large value ---\> for this effect size, high prob that the 2 models are practically the same.

-   Difference is nonzero, but small enough to not be practically meaningful

Use autoplot to compare each workflow to the current best model (random forest in this case)

```{r}
autoplot(rsq_anova, type = "ROPE", size = 0.02) +
  geom_text_repel(aes(label = workflow)) +
  theme(legend.position = "none")
```

Low pract_equiv for the other models means that they are much different from the random forest model.

### The Effect of the Amount of Resampling

More resamples --\> increased precision of the overall resampling estimate.

"The width of the intervals decreases as more resamples are added. Clearly, going from ten resamples to thirty has a larger impact than going from eighty to 100. There are diminishing returns for using a"large" number of resamples ("large" will be different for different data sets)."
