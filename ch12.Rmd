---
title: "Chapter 12"
author: "Brandie Quarles"
date: "`r Sys.Date()`"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 12 - Model Tuning and the Dangers of Overfitting

<https://www.tmwr.org/tuning>

Summary: "In tidymodels, the `tune()` function is used to identify parameters for optimization, and functions from the **dials** package can extract and interact with tuning parameters objects."

### Previous AMES Code:

```{r}
library(tidymodels)
tidymodels_prefer()
data(ames)
ames <- mutate(ames, Sale_Price = log10(Sale_Price))

set.seed(502)
ames_split <- initial_split(ames, prop = 0.80, strata = Sale_Price)
ames_train <- training(ames_split)
ames_test  <-  testing(ames_split)

ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train) %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = 0.01) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Latitude, Longitude, deg_free = 20)

lm_model <- linear_reg() %>% set_engine("lm")

lm_wflow <- 
  workflow() %>% 
  add_model(lm_model) %>% 
  add_recipe(ames_rec)

lm_fit <- fit(lm_wflow, ames_train)

rf_model <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("regression")

rf_wflow <- 
  workflow() %>% 
  add_formula(
    Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
      Latitude + Longitude) %>% 
  add_model(rf_model) 

set.seed(1001)
ames_folds <- vfold_cv(ames_train, v = 10)

keep_pred <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

set.seed(1003)
rf_res <- rf_wflow %>% fit_resamples(resamples = ames_folds, control = keep_pred)
```

## Model Parameters

Some parameters like beta0 and beta1 in linear regression can be estimated directly from the data.

Other parameters like K in the KNN model (number of neighbors (governs the flexibility of the class boundary) cannot be estimated directly from the data.

-   Question: Would the parameters in the Weibull model count for this? No b/c you can estimate it from the data itself

## Tuning Parameters for Different Types of Models

See section 12.2 for examples of different tuning parameters.

Not appropriate to tune the parameters for prior distributions in Bayesian analysis: "Our prior beliefs should not be subject to optimization. Tuning parameters are typically optimized for performance whereas priors should not be tweaked to get"the right results."

## What do we optimize?

Can use common statistical properties (like maximum likelihood or information criteria) if they are tractable for a given parameter

-   These properties may not align w/ results achieved using accuracy-oriented properties

Example with logistic regression (how to choose which link function to use)

```{r, eval=FALSE}
llhood <- function(...) {
  logistic_reg() %>% 
    set_engine("glm", ...) %>% 
    fit(Class ~ ., data = training_set) %>% 
    glance() %>% 
    select(logLik)
}

bind_rows(
  llhood(),
  llhood(family = binomial(link = "probit")),
  llhood(family = binomial(link = "cloglog"))
) %>% 
  mutate(link = c("logit", "probit", "c-log-log"))  %>% 
  arrange(desc(logLik))
#> # A tibble: 3 Ã— 2
#>   logLik link     
#>    <dbl> <chr>    
#> 1  -258. logit    
#> 2  -262. probit   
#> 3  -270. c-log-log
```

Hard to tell if the differences in log-likelihood values are important.

Use resampling to see if you get better answers (get 90% CIs)

```{r, eval=FALSE}
set.seed(1201)
rs <- vfold_cv(training_set, repeats = 10)

# Return the individual resampled performance estimates:
lloss <- function(...) {
  perf_meas <- metric_set(roc_auc, mn_log_loss)
    
  logistic_reg() %>% 
    set_engine("glm", ...) %>% 
    fit_resamples(Class ~ A + B, rs, metrics = perf_meas) %>% 
    collect_metrics(summarize = FALSE) %>%
    select(id, id2, .metric, .estimate)
}

resampled_res <- 
  bind_rows(
    lloss()                                    %>% mutate(model = "logistic"),
    lloss(family = binomial(link = "probit"))  %>% mutate(model = "probit"),
    lloss(family = binomial(link = "cloglog")) %>% mutate(model = "c-log-log")     
  ) %>%
  # Convert log-loss to log-likelihood:
  mutate(.estimate = ifelse(.metric == "mn_log_loss", -.estimate, .estimate)) %>% 
  group_by(model, .metric) %>% 
  summarize(
    mean = mean(.estimate, na.rm = TRUE),
    std_err = sd(.estimate, na.rm = TRUE) / sqrt(n()), 
    .groups = "drop"
  )

resampled_res %>% 
  filter(.metric == "mn_log_loss") %>% 
  ggplot(aes(x = mean, y = model)) + 
  geom_point() + 
  geom_errorbar(aes(xmin = mean - 1.64 * std_err, xmax = mean + 1.64 * std_err),
                width = .1) + 
  labs(y = NULL, x = "log-likelihood")
```

Can also calculate the area under the ROC curve for each resample.

Different metrics --\> different decisions about the choice of a tuning parameter

## The Consequences of Poor Parameter Estimates

OVERFITTING - where a model adapts too much to the training data and performs poorly for new data.

A lot of tuning parameters modulate the amount of model complexity --\> wrong choice (too much complexity) --\> overintepretation of chance patterns --\> overfitting.

Need to use resampling to see if a model is overfit to the training set (if more than 2 predictors, test set won't be sufficient).

## Two General Strategies for Optimization

1.  Grid Search - pre-define a set of parameter values to evaluate.

    1.  How to make the grid?

    2.  How many parameter combos to evaluate?

2.  Iterative Search - sequential search - sequentially discover new parameter combos based on previous results.

    1.  Can use any nonlinear optimization method

    2.  Some cases require an initial set of results for one or more parameter combos to start the optimization process

3.  Can use a hybrid method: grid search then sequential optimization can start from the best grid combo

## Tuning Parameters in TidyModels

Parsnip parameter arguments:

-   main arguments use a harmonized naming system to remove inconsistencies across engines

-   engine-specific arguments do not.

Parameters are marked for tuning by assigning them a value of `tune()`

```{r}
neural_net_spec <- 
  mlp(hidden_units = tune()) %>%
  set_mode("regression") %>%
  set_engine("keras")
```

To enumerate the tuning parameters for an object, use the `extract_parameter_set_dials()` function

nparam[+] indicates a numeric parameter

Tune the spline functions to have different levels of smoothness in AMES housing data:

```{r}
ames_rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + 
           Latitude + Longitude, data = ames_train)  %>%
  step_log(Gr_Liv_Area, base = 10) %>% 
  step_other(Neighborhood, threshold = tune()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_") ) %>% 
  step_ns(Longitude, deg_free = tune("longitude df")) %>% #give the tuning step an identifier 
  step_ns(Latitude,  deg_free = tune("latitude df"))

recipes_param <- extract_parameter_set_dials(ames_rec)
recipes_param
```

Can combine a recipe and model specification (both with tuning) into a single workflow:

```{r}
wflow_param <- 
  workflow() %>% 
  add_recipe(ames_rec) %>% 
  add_model(neural_net_spec) %>% 
  extract_parameter_set_dials()
wflow_param
```

### Use dials package for tuning functions

Usually the same name as the parameter argument:

```{r}
hidden_units()

threshold()
```

Sometimes different (like degrees of freedom):

```{r}
spline_degree()
```

Extract a particular parameter object:

```{r}
# identify the parameter using the id value:
wflow_param %>% extract_parameter_dials("threshold")
```

Update the range of parameters inside the parameter set:

```{r}
extract_parameter_set_dials(ames_rec) %>% 
  update(threshold = threshold(c(0.8, 1.0)))
```

Parameter Range defaults can either be easy or need input from you.

Example, random forest \# of predictor columns that are randomly sampled for each split in the tree (need to input the number of predictors):

```{r}
rf_spec <- 
  rand_forest(mtry = tune()) %>% 
  set_engine("ranger", regularization.factor = tune("regularization")) %>%
  set_mode("regression")

rf_param <- extract_parameter_set_dials(rf_spec)
rf_param
```

Complete parameter objects have `[+]` in their summary; a value of `[?]` indicates that at least one end of the possible range is missing. There are two methods for handling this.

Can use update to fix a [?]:

```{r}
rf_param %>% 
  update(mtry = mtry(c(1, 70)))
```

"However, this approach might not work if a recipe is attached to a workflow that uses steps that either add or subtract columns. If those steps are not slated for tuning, the `finalize()` function can execute the recipe once to obtain the dimensions:"

```{r}
pca_rec <- 
  recipe(Sale_Price ~ ., data = ames_train) %>% 
  # Select the square-footage predictors and extract their PCA components:
  step_normalize(contains("SF")) %>% 
  # Select the number of components needed to capture 95% of
  # the variance in the predictors. 
  step_pca(contains("SF"), threshold = .95)
  
updated_param <- 
  workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(pca_rec) %>% 
  extract_parameter_set_dials() %>% 
  finalize(ames_train) #finalize function 
updated_param

updated_param %>% extract_parameter_dials("mtry")
```

Some tuning parameters have a default data transformation:

```{r}
penalty() #log10 transformation included
```

This is important to know, especially when altering the range. New range values must be in the transformed units:

```{r}
# correct method to have penalty values between 0.1 and 1.0
penalty(c(-1, 0)) %>% value_sample(1000) %>% summary()

# incorrect:
penalty(c(0.1, 1.0)) %>% value_sample(1000) %>% summary()

```

The scale can be changed if desired with the `trans` argument. You can use natural units but the same range:

```{r}
penalty(trans = NULL, range = 10^c(-10, 0))
```
